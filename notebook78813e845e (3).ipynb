{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"GANs are unsupervised learning algorithms with supervised loss.Is this statement correct?","metadata":{"papermill":{"duration":0.032771,"end_time":"2022-01-11T10:40:10.637259","exception":false,"start_time":"2022-01-11T10:40:10.604488","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"CycleGAN instead of normal GAN because the problem we are dealing here has data of unpaired type","metadata":{"papermill":{"duration":0.042507,"end_time":"2022-01-11T10:40:10.720198","exception":false,"start_time":"2022-01-11T10:40:10.677691","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**IMPORTS**","metadata":{"papermill":{"duration":0.05759,"end_time":"2022-01-11T10:40:10.817177","exception":false,"start_time":"2022-01-11T10:40:10.759587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install git+https://github.com/tensorflow/examples.git\n# to import generator and discriminator    ","metadata":{"id":"bJ1ROiQxJ-vY","outputId":"f3764b1f-0b72-4df7-b18b-132a634b7cb0","papermill":{"duration":16.880883,"end_time":"2022-01-11T10:40:27.75302","exception":false,"start_time":"2022-01-11T10:40:10.872137","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:12.08998Z","iopub.execute_input":"2022-01-23T08:40:12.090636Z","iopub.status.idle":"2022-01-23T08:40:32.755495Z","shell.execute_reply.started":"2022-01-23T08:40:12.090516Z","shell.execute_reply":"2022-01-23T08:40:32.754497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"id":"lhSsUx9Nyb3t","papermill":{"duration":3.743436,"end_time":"2022-01-11T10:40:31.522914","exception":false,"start_time":"2022-01-11T10:40:27.779478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:32.759564Z","iopub.execute_input":"2022-01-23T08:40:32.759901Z","iopub.status.idle":"2022-01-23T08:40:37.872078Z","shell.execute_reply.started":"2022-01-23T08:40:32.759871Z","shell.execute_reply":"2022-01-23T08:40:37.870991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"id":"YfIk2es3hJEd","papermill":{"duration":1.876375,"end_time":"2022-01-11T10:40:33.424652","exception":false,"start_time":"2022-01-11T10:40:31.548277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:37.874Z","iopub.execute_input":"2022-01-23T08:40:37.874295Z","iopub.status.idle":"2022-01-23T08:40:40.584559Z","shell.execute_reply.started":"2022-01-23T08:40:37.874254Z","shell.execute_reply":"2022-01-23T08:40:40.583556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import listdir","metadata":{"id":"XEwfPFXmrtod","papermill":{"duration":0.030835,"end_time":"2022-01-11T10:40:33.482544","exception":false,"start_time":"2022-01-11T10:40:33.451709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:40.58734Z","iopub.execute_input":"2022-01-23T08:40:40.5888Z","iopub.status.idle":"2022-01-23T08:40:40.596155Z","shell.execute_reply.started":"2022-01-23T08:40:40.588726Z","shell.execute_reply":"2022-01-23T08:40:40.592418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###                                                          CALLING IMAGE_DATA_GENERATOR FOR SETTING UP INPUT PIPELINE\n\n\n\n","metadata":{"id":"zykLUcP3OPg2","papermill":{"duration":0.02507,"end_time":"2022-01-11T10:40:33.532718","exception":false,"start_time":"2022-01-11T10:40:33.507648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator()\n\n#function is called here but iterators are called where we write training step\n#ImageDataGenerator helps us in efficiently reading data as it releases previous batch from RAM before reading next batch","metadata":{"id":"O3dE8kQFOMQG","papermill":{"duration":0.031504,"end_time":"2022-01-11T10:40:33.589297","exception":false,"start_time":"2022-01-11T10:40:33.557793","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:40.598791Z","iopub.execute_input":"2022-01-23T08:40:40.601974Z","iopub.status.idle":"2022-01-23T08:40:40.62237Z","shell.execute_reply.started":"2022-01-23T08:40:40.60191Z","shell.execute_reply":"2022-01-23T08:40:40.621102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_A = datagen.flow_from_directory('/content/A', class_mode='binary',batch_size=10)\n# train_B = datagen.flow_from_directory('/content/B', class_mode='binary',batch_size=10)","metadata":{"id":"KVYhpkxCOMNQ","outputId":"6e512a44-c41a-4db4-fcdd-9a58c9cff80a","papermill":{"duration":0.030957,"end_time":"2022-01-11T10:40:33.645353","exception":false,"start_time":"2022-01-11T10:40:33.614396","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:40.624834Z","iopub.execute_input":"2022-01-23T08:40:40.625584Z","iopub.status.idle":"2022-01-23T08:40:40.633484Z","shell.execute_reply.started":"2022-01-23T08:40:40.625491Z","shell.execute_reply":"2022-01-23T08:40:40.632437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------------------------------------------","metadata":{"id":"CcQ-LdVuObxM","papermill":{"duration":0.025061,"end_time":"2022-01-11T10:40:33.695901","exception":false,"start_time":"2022-01-11T10:40:33.67084","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---------------------------------------------------------------------------------------","metadata":{"id":"jdybJaWnOM4O","papermill":{"duration":0.025015,"end_time":"2022-01-11T10:40:33.746131","exception":false,"start_time":"2022-01-11T10:40:33.721116","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**ALTERNATE WAY OF TAKING IMAGE INPUT BUT FOR LARGER DATASET RESULTED INT RAM OVERFLOW**","metadata":{"papermill":{"duration":0.025093,"end_time":"2022-01-11T10:40:33.79652","exception":false,"start_time":"2022-01-11T10:40:33.771427","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"TAKING TESTDATA AS INPUT","metadata":{"papermill":{"duration":0.024946,"end_time":"2022-01-11T10:40:33.846652","exception":false,"start_time":"2022-01-11T10:40:33.821706","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Though this data takes input from stream still it overflowed RAM on larger dataset but since train dataset is\n# small we can use this as well but try replacing this by image data generator\nBUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"id":"2CbTEt448b4R","papermill":{"duration":0.031743,"end_time":"2022-01-11T10:40:33.903465","exception":false,"start_time":"2022-01-11T10:40:33.871722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:40.635362Z","iopub.execute_input":"2022-01-23T08:40:40.635914Z","iopub.status.idle":"2022-01-23T08:40:40.644765Z","shell.execute_reply.started":"2022-01-23T08:40:40.635871Z","shell.execute_reply":"2022-01-23T08:40:40.643649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames=list()\nlabels=list()\npath='../input/trails/Sprites/'\nfor filename in listdir(path):\n   filenames.append(filename)\n   labels.append(1)\n\ndataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\ndef _parse_function(filename, label):\n    image_string = tf.io.read_file(path+filename)\n    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image_decoded, tf.float32)\n    return image, label\n\ndataset = dataset.map(_parse_function)","metadata":{"id":"db2Cvjfq5jhK","papermill":{"duration":2.772049,"end_time":"2022-01-11T10:40:36.700903","exception":false,"start_time":"2022-01-11T10:40:33.928854","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:40.646246Z","iopub.execute_input":"2022-01-23T08:40:40.647254Z","iopub.status.idle":"2022-01-23T08:40:43.626079Z","shell.execute_reply.started":"2022-01-23T08:40:40.647208Z","shell.execute_reply":"2022-01-23T08:40:43.624976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**------------------------------------------------------------------------------------------------------------------**","metadata":{"papermill":{"duration":0.026127,"end_time":"2022-01-11T10:40:36.754443","exception":false,"start_time":"2022-01-11T10:40:36.728316","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**-------------------------------------------------------------------------------------------------------------------**","metadata":{"papermill":{"duration":0.025735,"end_time":"2022-01-11T10:40:36.806354","exception":false,"start_time":"2022-01-11T10:40:36.780619","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# **MODEL CREATION**","metadata":{"papermill":{"duration":0.025829,"end_time":"2022-01-11T10:40:36.858477","exception":false,"start_time":"2022-01-11T10:40:36.832648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install --upgrade keras","metadata":{"id":"tKM29s86SmeM","outputId":"57a26fe2-d5c7-483e-ce97-c3431993c3c9","papermill":{"duration":9.955766,"end_time":"2022-01-11T10:40:46.840577","exception":false,"start_time":"2022-01-11T10:40:36.884811","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:43.627796Z","iopub.execute_input":"2022-01-23T08:40:43.628347Z","iopub.status.idle":"2022-01-23T08:40:56.109551Z","shell.execute_reply.started":"2022-01-23T08:40:43.6283Z","shell.execute_reply":"2022-01-23T08:40:56.108448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing generator and discriminator**","metadata":{"papermill":{"duration":0.029949,"end_time":"2022-01-11T10:40:46.901063","exception":false,"start_time":"2022-01-11T10:40:46.871114","status":"completed"},"tags":[]}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\nfrom tensorflow_examples.models.pix2pix import pix2pix\n# In Instance Normalization -> mean and variance are calculated for each individual channel for each individual sample\nGen_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\nGen_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\nDisc_y = pix2pix.discriminator(norm_type='instancenorm', target=False)\nDisc_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n\n\n# simple unet generator and  unet discriminators are used\n# in original implementation unet generator and resnet discriminator is used\n\n# insted of batch, instance normalisation is used (as per paper)","metadata":{"id":"8ju9Wyw87MRW","papermill":{"duration":1.455934,"end_time":"2022-01-11T10:40:48.387324","exception":false,"start_time":"2022-01-11T10:40:46.93139","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:56.11424Z","iopub.execute_input":"2022-01-23T08:40:56.116665Z","iopub.status.idle":"2022-01-23T08:40:57.996278Z","shell.execute_reply.started":"2022-01-23T08:40:56.116631Z","shell.execute_reply":"2022-01-23T08:40:57.995183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Setting up all loss functions**","metadata":{"papermill":{"duration":0.030028,"end_time":"2022-01-11T10:40:48.448104","exception":false,"start_time":"2022-01-11T10:40:48.418076","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LAMBDA = 10\n#to be multiplied with cyclic loss as per paper","metadata":{"id":"cyhxTuvJyIHV","papermill":{"duration":0.036279,"end_time":"2022-01-11T10:40:48.514301","exception":false,"start_time":"2022-01-11T10:40:48.478022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:57.997666Z","iopub.execute_input":"2022-01-23T08:40:57.99795Z","iopub.status.idle":"2022-01-23T08:40:58.00228Z","shell.execute_reply.started":"2022-01-23T08:40:57.997909Z","shell.execute_reply":"2022-01-23T08:40:58.001395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Binary cross entropy loss is considered (for losses of both GANs)\nloss_BinaryCross = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"id":"Q1Xbz5OaLj5C","papermill":{"duration":0.036373,"end_time":"2022-01-11T10:40:48.580716","exception":false,"start_time":"2022-01-11T10:40:48.544343","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.004297Z","iopub.execute_input":"2022-01-23T08:40:58.004863Z","iopub.status.idle":"2022-01-23T08:40:58.016813Z","shell.execute_reply.started":"2022-01-23T08:40:58.004808Z","shell.execute_reply":"2022-01-23T08:40:58.015715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generator and Discriminator losses are calculated in same way as in tutorial of GANs\n![https://drive.google.com/file/d/1pz4em0aasuUIJvZnQmL_urlNBl1qUJTe/view?usp=sharing]","metadata":{"papermill":{"duration":0.030105,"end_time":"2022-01-11T10:40:48.640926","exception":false,"start_time":"2022-01-11T10:40:48.610821","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n  real_loss = loss_BinaryCross(tf.ones_like(real), real)\n  generated_loss = loss_BinaryCross(tf.zeros_like(generated), generated)\n  total_disc_loss = real_loss + generated_loss\n  return total_disc_loss * 0.5\n\ndef generator_loss(generated):\n  return loss_BinaryCross(tf.ones_like(generated), generated)\n\n# cyclic loss to ensure mapping can be established between two GANs\ndef calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  return LAMBDA * loss1\n\ndef identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * loss *0.5","metadata":{"id":"wkMNfBWlT-PV","papermill":{"duration":0.03927,"end_time":"2022-01-11T10:40:48.710264","exception":false,"start_time":"2022-01-11T10:40:48.670994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.020746Z","iopub.execute_input":"2022-01-23T08:40:58.021031Z","iopub.status.idle":"2022-01-23T08:40:58.030333Z","shell.execute_reply.started":"2022-01-23T08:40:58.020962Z","shell.execute_reply":"2022-01-23T08:40:58.029256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Optimisers : For BackPropagation on gradients and modifying weights**","metadata":{"papermill":{"duration":0.030605,"end_time":"2022-01-11T10:40:48.771028","exception":false,"start_time":"2022-01-11T10:40:48.740423","status":"completed"},"tags":[]}},{"cell_type":"code","source":"Gen_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nGen_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nDisc_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nDisc_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"id":"iWCn_PVdEJZ7","papermill":{"duration":0.037804,"end_time":"2022-01-11T10:40:48.839338","exception":false,"start_time":"2022-01-11T10:40:48.801534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.032039Z","iopub.execute_input":"2022-01-23T08:40:58.032791Z","iopub.status.idle":"2022-01-23T08:40:58.041032Z","shell.execute_reply.started":"2022-01-23T08:40:58.032746Z","shell.execute_reply":"2022-01-23T08:40:58.040138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RESTORING THE LAST CHECKPOINT :**","metadata":{"papermill":{"duration":0.030859,"end_time":"2022-01-11T10:40:48.900515","exception":false,"start_time":"2022-01-11T10:40:48.869656","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Upload your checkpoints as datasets and enter their path here\n#Most recent checkpoints will be loaded\n#If no checkpoints exists this wont show error\ncheckpoint_path = \"../input/checkpoint-10-epochs-largerdataset/\"\n\nckpt = tf.train.Checkpoint(Gen_g=Gen_g,\n                           Gen_f=Gen_f,\n                           Disc_x=Disc_x,\n                           Disc_y=Disc_y,\n                           Gen_g_optimizer=Gen_g_optimizer,\n                           Gen_f_optimizer=Gen_f_optimizer,\n                           Disc_x_optimizer=Disc_x_optimizer,\n                           Disc_y_optimizer=Disc_y_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')","metadata":{"id":"WJnftd5sQsv6","papermill":{"duration":0.04134,"end_time":"2022-01-11T10:40:48.972215","exception":false,"start_time":"2022-01-11T10:40:48.930875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.043043Z","iopub.execute_input":"2022-01-23T08:40:58.043424Z","iopub.status.idle":"2022-01-23T08:40:58.055545Z","shell.execute_reply.started":"2022-01-23T08:40:58.043349Z","shell.execute_reply":"2022-01-23T08:40:58.054507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL**","metadata":{"papermill":{"duration":0.03047,"end_time":"2022-01-11T10:40:49.034002","exception":false,"start_time":"2022-01-11T10:40:49.003532","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**generate_images -> to generate image with current weights of generator**\n\nUsed while generating on test data","metadata":{"papermill":{"duration":0.030232,"end_time":"2022-01-11T10:40:49.094725","exception":false,"start_time":"2022-01-11T10:40:49.064493","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def generate_images(model, test_input):\n  prediction = model(test_input)\n    \n  plt.figure(figsize=(12, 12))\n\n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Image']\n\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()","metadata":{"id":"RmdVsmvhPxyy","papermill":{"duration":0.038616,"end_time":"2022-01-11T10:40:49.164156","exception":false,"start_time":"2022-01-11T10:40:49.12554","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.05723Z","iopub.execute_input":"2022-01-23T08:40:58.057699Z","iopub.status.idle":"2022-01-23T08:40:58.06911Z","shell.execute_reply.started":"2022-01-23T08:40:58.05765Z","shell.execute_reply":"2022-01-23T08:40:58.068079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Logic For Training = True**\n\nSome neural network layers behave differently during training and inference, for example Dropout and BatchNormalization layers. For example\nDuring training, dropout will randomly drop out units and correspondingly scale up activations of the remaining units.\nDuring inference, it does nothing (since you usually don't want the randomness of dropping out units here).\nThe training argument lets the layer know which of the two \"paths\" it should take. If you set this incorrectly, your network might not behave as expected.","metadata":{"papermill":{"duration":0.030653,"end_time":"2022-01-11T10:40:49.225736","exception":false,"start_time":"2022-01-11T10:40:49.195083","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_normal,real_pixel):\n  #Gradient tape records the operations on variables being watched and allow us to perform automatic differentiation\n  with tf.GradientTape(persistent=True) as tape:\n    #pixel = Y and normal=X\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n    \n    fake_pixel = Gen_g(real_normal, training=True)\n    cycled_normal = Gen_f(fake_pixel, training=True)\n\n    fake_normal = Gen_f(real_pixel, training=True)\n    cycled_pixel = Gen_g(fake_normal, training=True)\n\n    #  for identity loss. -> if image of target class is given as input it should return same (equivalent) image\n    same_normal = Gen_f(real_normal, training=True)\n    same_pixel = Gen_g(real_pixel, training=True)\n\n    disc_real_normal = Disc_x(real_normal, training=True)\n    disc_real_pixel = Disc_y(real_pixel, training=True)\n\n    disc_fake_normal = Disc_x(fake_normal, training=True)\n    disc_fake_pixel = Disc_y(fake_pixel, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_pixel)\n    gen_f_loss = generator_loss(disc_fake_normal)\n    \n    total_cycle_loss = calc_cycle_loss(real_pixel, cycled_pixel) + calc_cycle_loss(real_normal, cycled_normal)\n    \n    # Total generator loss = adversarial loss + cycle loss\n    #identity loss is added because we want to have similarities bw input and generated images as close as possible\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_pixel,same_pixel)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_normal,same_normal)\n\n    disc_x_loss = discriminator_loss(disc_real_normal, disc_fake_normal)\n    disc_y_loss = discriminator_loss(disc_real_pixel, disc_fake_pixel)\n  \n  \n  # Calculate --> gradients for generator and discriminator  (tape.gradient(y,x) implies dy/dx)\n  Gen_g_gradients = tape.gradient(total_gen_g_loss, \n                                        Gen_g.trainable_variables)\n  Gen_f_gradients = tape.gradient(total_gen_f_loss, \n                                        Gen_f.trainable_variables)\n  \n  Disc_x_gradients = tape.gradient(disc_x_loss, \n                                            Disc_x.trainable_variables)\n  Disc_y_gradients = tape.gradient(disc_y_loss, \n                                            Disc_y.trainable_variables)\n  \n  # Apply the gradients to the optimizer (gradient descent to update the weights)\n  # the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. thats why persistent is set to True because the tape is used more than ones\n  Gen_g_optimizer.apply_gradients(zip(Gen_g_gradients, \n                                            Gen_g.trainable_variables))\n\n  Gen_f_optimizer.apply_gradients(zip(Gen_f_gradients, \n                                            Gen_f.trainable_variables))\n  \n  Disc_x_optimizer.apply_gradients(zip(Disc_x_gradients,\n                                                Disc_x.trainable_variables))\n  \n  Disc_y_optimizer.apply_gradients(zip(Disc_y_gradients,\n                                                Disc_y.trainable_variables))","metadata":{"id":"KBKUV2sKXDbY","papermill":{"duration":0.045423,"end_time":"2022-01-11T10:40:49.301735","exception":false,"start_time":"2022-01-11T10:40:49.256312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-23T08:40:58.071085Z","iopub.execute_input":"2022-01-23T08:40:58.071411Z","iopub.status.idle":"2022-01-23T08:40:58.086343Z","shell.execute_reply.started":"2022-01-23T08:40:58.071368Z","shell.execute_reply":"2022-01-23T08:40:58.085356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAINING STEP:**","metadata":{"papermill":{"duration":0.030446,"end_time":"2022-01-11T10:40:49.362907","exception":false,"start_time":"2022-01-11T10:40:49.332461","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\ndef preprocess_image_train(image):\n  #jitter\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  # randomly cropping to 256 x 256 x 3\n  image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n  #   normalise\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image\n\n\n# 1 epoch -> 1 time model has seen entire dataset\n# 1 iteration -> model has seen 1 batch of data and has updated weights after seeing that batch of data\n#Batch size of 10 is considered here \nepochs=10\nfor epoch in range (0,epochs):\n    start=time.time()  \n    train_A = datagen.flow_from_directory('../input/aimages/A', class_mode='binary',batch_size=10)\n    train_B = datagen.flow_from_directory('../input/pixeldataset/pixeldataset', class_mode='binary',batch_size=10)\n    a=train_A.next()\n    b=train_B.next()\n    i=0\n    while a and b:\n       if i>500 :\n          break\n       i+=1\n       j=0\n       for j in range (0,len(a[0])):\n         a[0][j]=preprocess_image_train(a[0][j])\n       for j in range (0,len(b[0])):\n         b[0][j]=preprocess_image_train(b[0][j])\n       train_step(a[0],b[0])\n       a=train_A.next()  #fetching iterator to next batch of image A\n       b=train_B.next()  #fetching iterator to next batch of image B\n\n    print('Time taken for epoch {} is {} sec\\n'.format(epoch,time.time()-start))\n    if epoch % 2 == 0:\n      checkpoint_path = \"./checkpoints/train\"\n      ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n      ckpt_save_path = ckpt_manager.save()\n      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n      clear_output(wait=True)","metadata":{"id":"KtcMlt9MfRDK","outputId":"f3b8996f-2e6a-450c-a322-5ec5cd1f05e4","papermill":{"duration":4470.38654,"end_time":"2022-01-11T11:55:19.780093","exception":false,"start_time":"2022-01-11T10:40:49.393553","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TESTING STEP:**","metadata":{"papermill":{"duration":0.032028,"end_time":"2022-01-11T11:55:19.845331","exception":false,"start_time":"2022-01-11T11:55:19.813303","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_test(image, label):\n  image = tf.image.resize(image,[256,256])\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image\ntrain_A = dataset.cache().map(\n    preprocess_test, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\nfor inp in train_A.take(100):\n  generate_images(Gen_g, inp)","metadata":{"id":"XqG9F3kBoxB9","outputId":"37b8db89-56d3-41fa-9161-0d0f33011245","papermill":{"duration":41.067815,"end_time":"2022-01-11T11:56:00.944845","exception":false,"start_time":"2022-01-11T11:55:19.87703","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**What can be done to improve results:->**\n\n**1)Obtain better dataset  (By scrapping websites)**\n\n**2)experimenting with hyper paramenters (batch size,lambda,alpha(learning rate),epochs etc)**\n\n**3)Trying different models for generator and discriminator (like resnet etc)**\n\n**4)Trying different optimisers**\n\n**5)Setting up some sort of evaluation of model**","metadata":{"papermill":{"duration":0.764514,"end_time":"2022-01-11T11:56:02.513182","exception":false,"start_time":"2022-01-11T11:56:01.748668","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.771402,"end_time":"2022-01-11T11:56:04.045885","exception":false,"start_time":"2022-01-11T11:56:03.274483","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}